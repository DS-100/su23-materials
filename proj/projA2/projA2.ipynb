{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"projA2.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0f9b2de18190d9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Project A2: Predicting Housing Prices in Cook County\n",
    "\n",
    "## Due Date: Monday, July 24th, 11:59 PM\n",
    "\n",
    "You must submit this assignment to Gradescope by the on-time deadline, Monday, July 24th, 11:59 PM. Please read the syllabus for the grace period policy. No late submissions beyond the grace period will be accepted. While course staff is happy to help you if you encounter difficulties with submission, we may not be able to respond to last-minute requests for assistance (TAs need to sleep, after all!). **We strongly encourage you to plan to submit your work to Gradescope several hours before the stated deadline.** This way, you will have ample time to reach out to staff for submission support. \n",
    "\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the project, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the collaborators cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *list names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In project A1, you performed some basic Exploratory Data Analysis (EDA), laying out the thought process that leads to certain modeling decisions. Then, you added a few new features to the dataset and cleaned the data in the process.\n",
    "\n",
    "In this project, you will specify and fit a linear model to a few features of the housing data to predict house prices. Next, we will analyze the error of the model and brainstorm ways to improve the model's performance. Finally, we'll delve deeper into the implications of predictive modeling within the Cook County Assessor's Office (CCAO) case study, especially because statistical modeling is how the CCAO valuates properties. Given the history of racial discrimination in housing policy and property taxation in Cook County, consider the impacts of your modeling results as you work through this project - and think about what fairness might mean to property owners in Cook County.\n",
    "\n",
    "After this part of the project, you should be comfortable with:\n",
    "- Implementing a data processing pipeline using `pandas`.\n",
    "- Using `scikit-learn` to build and fit linear models.\n",
    "\n",
    "## Score Breakdown\n",
    "\n",
    "Question | Manual | Points\n",
    "----|----|----\n",
    "00 | No| 2\n",
    "0a | Yes | 1\n",
    "0b | Yes | 1\n",
    "0c | No | 1\n",
    "0d | Yes | 1\n",
    "0e | Yes | 1\n",
    "1 | No | 2\n",
    "2a | Yes | 2\n",
    "2b | No | 3\n",
    "2c | No | 2\n",
    "3a | Yes | 2\n",
    "3b | No | 1\n",
    "4a | No | 0\n",
    "4b | No | 0\n",
    "4c | No | 0\n",
    "4d | No | 0\n",
    "4e | No | 3\n",
    "Test Prediction | No | 3\n",
    "4f | No | 0\n",
    "4g | No | 0\n",
    "4h | No | 0\n",
    "5a | No | 0\n",
    "5b | No | 0\n",
    "5c | No | 0\n",
    "Total | 6 | 26\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62cfd21463535cac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from ds100_utils import run_linear_regression_test\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the training/validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('cook_county_data.zip') as item:\n",
    "    item.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is split into a training/validation set and a testing set. Importantly, the test set does not contain values for our target variable, `Sale Price`.  In this project, you will train a model on the training/validation set, then use this trained model to predict the `Sale Price`s of the test set. In the cell below, we load the training/validation set into the DataFrame `training_val_data` and the test set into the DataFrame `test_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8fea30adc9d489b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "training_val_data = pd.read_csv(\"cook_county_train_val.csv\", index_col='Unnamed: 0')\n",
    "test_data = pd.read_csv(\"cook_county_contest_test.csv\", index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d6d509b6e854e10",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "As a good sanity check, we should at least verify that the data shape matches the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c841a2de55691502",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 204792 observations and 62 features in training data\n",
    "assert training_val_data.shape == (204792, 62)\n",
    "# 55311 observations and 61 features in test data\n",
    "assert test_data.shape == (55311, 61)\n",
    "# Sale Price is provided in the training/validation data\n",
    "assert 'Sale Price' in training_val_data.columns.values\n",
    "# Sale Price is hidden in the test data\n",
    "assert 'Sale Price' not in test_data.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce9acc2f62c96e59",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's remind ourselves of the data available to us in the Cook County dataset. Remember, a more detailed description of each variable is included in `codebook.txt`, which is in the same directory as this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e60a7a0cda5eecf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "training_val_data.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 0: Human Context and Ethics\n",
    "\n",
    "In this part of the project, we will explore the human context of our housing dataset. **You should watch Lecture 14 before attempting this part.**\n",
    "\n",
    "<br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "### Question 0a\n",
    "\"How much is a house worth?\" Who might be interested in an answer to this question? **Please list at least three different parties (people or organizations) and state whether each one has an interest in seeing the housing price to be high or low.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "### Question 0b\n",
    "\n",
    "Which of the following scenarios strike you as unfair and why? You can choose more than one. There is no single right answer, but you must explain your reasoning. Would you consider some of these scenarios more (or less) fair than others? Why?\n",
    "\n",
    "A. A homeowner whose home is assessed at a higher price than it would sell for.  \n",
    "B. A homeowner whose home is assessed at a lower price than it would sell for.  \n",
    "C. An assessment process that systematically overvalues inexpensive properties and undervalues expensive properties.  \n",
    "D. An assessment process that systematically undervalues inexpensive properties and overvalues expensive properties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "### Question 0c\n",
    "\n",
    "Consider a model that is fit to $n = 30$ training observations. Call the response $y$ (Log Sale Price), the predictions $\\hat{y}$, and the residuals $y - \\hat{y}$. Which of the following residual plots of $y$ versus $y - \\hat{y}$ correspond to a model that might make property assessments that result in to regressive taxation? Recall from Lecture 14 that regressive taxation overvalues inexpensive properties and undervalues expensive properties. Assume that all three plots use the same vertical scale and that the horizontal line marks $y - \\hat{y} = 0$. Assign `q0c` to the string letter corresponding to your choice of plot.\n",
    "\n",
    "Hint: When a model overvalues a property (predicts a `Sale Price` greater than the true `Sale Price`) , what are the relative sizes of $y$ and $\\hat{y}$? What about when a model undervalues a property?\n",
    "\n",
    "![](res-plots.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q0c = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q0c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CCAO Dataset\n",
    "\n",
    "The dataset you’ll be working with comes from the Cook County Assessor’s Office (CCAO) in Illinois, a government institution that determines property taxes across most of Chicago’s metropolitan area and its nearby suburbs. In the United States, all property owners are required to pay property taxes, which are then used to fund public services including education, road maintenance, and sanitation. These property tax assessments are based on property values estimated using statistical models that consider multiple factors, such as real estate value and construction cost.\n",
    "\n",
    "This system, however, is not without flaws. In late 2017, a lawsuit was filed against the office of Cook County Assessor Joseph Berrios for producing “[racially discriminatory assessments and taxes](https://www.chicagotribune.com/politics/ct-cook-county-board-assessor-berrios-met-20170718-story.html).\" The lawsuit included claims that the assessor’s office undervalued high-priced homes and overvalued low-priced homes, creating a visible divide along racial lines: Wealthy homeowners, who were typically white, [paid less in property taxes](https://www.clccrul.org/bpnc-v-berrios-facts?rq=berrios), whereas [working-class, non-white homeowners paid more](https://www.chicagotribune.com/news/breaking/ct-cook-county-assessor-berrios-sued-met-20171214-story.html).\n",
    "\n",
    "The Chicago Tribune's four-part series, \"[The Tax Divide](https://www.chicagotribune.com/investigations/ct-tax-divide-investigation-20180425-storygallery.html)\", delves into how this was uncovered: After \"compiling and analyzing more than 100 million property tax records from the years 2003 through 2015, along with thousands of pages of documents, then vetting the findings with top experts in the field,\" they discovered that \"residential assessments had been so far off the mark for so many years.\" You can read more about their investigation [here](https://apps.chicagotribune.com/news/watchdog/cook-county-property-tax-divide/assessments.html).\n",
    "\n",
    "And make sure to watch Lecture 14 before answering the following questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "### Question 0d\n",
    "\n",
    "What were the central problems with the earlier property tax system in Cook County as reported by the Chicago Tribune ? And what were the primary causes of these problems? (Note: in addition to reading the paragraph above you will need to watch the lecture to answer this question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "### Question 0e\n",
    "\n",
    "In addition to being regressive, how did the property tax system in Cook County place a disproportionate tax burden on non-white property owners?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 1: Preparing Data\n",
    "\n",
    "\n",
    "Let's split the dataset into a training set and validation set. We will use the training set to fit our model's parameters, and we will use the validation set to evaluate how well our model will perform on unseen data drawn from the same distribution. If we used all the data to fit our model, we would not have a way to estimate model performance on **unseen data** such as the test set in `cook_county_contest_test.csv`.\n",
    "\n",
    "In the cell below, complete the function `train_val_split` that splits `data` into two smaller DataFrames named `train` and `validation`. Let `train` contain 80% of the data, and let `validation` contain the remaining 20% of the data. You should not be importing any additional libraries for this question. Your answer should use the variable `shuffled_indices` defined for you. Take a look at the`np.permutation` [documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.permutation.html). You should only be using numpy functions to generate randomness!\n",
    "\n",
    "**Note**: \n",
    "\n",
    "You may see that some sources uses the name holdout, validation, or test synonymously since all of them are not used for training directly. There is no difference between holdout and validation set, both can be used for hyperparameter tuning. However, test set should only be used for final unbiased evaluation.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "While there are multiple solutions, one way is to create two `NumPy` arrays named `train_indices` and `validation_indices` (or any variable names of your choice) that contain a *random* 80% of the indices in `full_data`, and the remaining 20% of the indices, respectively. Then, use these arrays to index into `data` to create your final `train` and `validation` DataFrames. To ensure that your code matches to our solution, use the first 80% as the the training set and the last 20% as the validation set. \n",
    "\n",
    "*The provided tests check that you not only answered correctly, but ended up with the exact same train/validation split as our reference implementation. Later testing is easier this way.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This makes the train-validation split in this section reproducible across different runs \n",
    "# of the notebook. You do not need this line to run train_val_split in general\n",
    "\n",
    "# DO NOT CHANGE THIS LINE\n",
    "np.random.seed(1337)\n",
    "# DO NOT CHANGE THIS LINE\n",
    "\n",
    "def train_val_split(data):\n",
    "    \"\"\" \n",
    "    Takes in a dataframe `data` and randomly splits it into two smaller DataFrames \n",
    "    named `train` and `validation` with 80% and 20% of the data respectively. \n",
    "    \"\"\"\n",
    "    \n",
    "    data_len = data.shape[0]\n",
    "    shuffled_indices = np.random.permutation(data_len)\n",
    "    ...\n",
    "    train = ...\n",
    "    validation = ...\n",
    "   \n",
    "    return train, validation\n",
    "train, validation = train_val_split(training_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2: Fitting a Simple Model\n",
    "\n",
    "Now, let's fit our updated linear regression model using the ordinary least squares estimator! We will start you off with something simple by using only 2 features: the **number of bedrooms** in the household and the **log-transformed total area covered by the building** (in square feet). \n",
    "\n",
    "Consider the following expression for our 1st linear model that contains one of the features:\n",
    "\n",
    "$$\n",
    "\\text{Log Sale Price} = \\theta_0 + \\theta_1 \\cdot (\\text{Bedrooms})\n",
    "$$\n",
    "\n",
    "In parallel, we will also consider a 2nd model that contains both features:\n",
    "\n",
    "$$\n",
    "\\text{Log Sale Price} = \\theta_0 + \\theta_1 \\cdot (\\text{Bedrooms}) + \\theta_2 \\cdot (\\text{Log Building Square Feet})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 2a\n",
    "\n",
    "**Without running any calculation or code**, assign `q2a` to be the comparator ('>=', '=', '<=') that fills the blank in the following statement:\n",
    "\n",
    "Suppose we quantify the loss on our linear models using MSE (Mean Squared Error). Consider the training loss of the 1st model and the training loss of the 2nd model. We are guaranteed that:\n",
    "\n",
    "$$\n",
    "\\text{Training Loss of the 1st Model}  \\_\\_\\_\\_\\_  \\text{Training Loss of the 2nd Model}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q2a = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### Pipeline function\n",
    "\n",
    "In A1, you wrote a few functions that added features to the dataset. Instead of calling them manually one-by-one each time, it is best practice to encapsulate all of this feature engineering into one \"pipeline\" function. Defining and using a pipeline reduces all the feature engineering to just one function call and ensures that the same transformations are applied to all data.  Below, we combined some functions into a single helper function that outputs X and y for the first model above. Try to understand what this function does! \n",
    "\n",
    "**Note:** We have automatically imported staff implementations of the functions you wrote in Project A1. These functions are `remove_outliers`, `add_total_bedrooms`, `find_expensive_neighborhoods`, `add_in_expensive_neighborhood`, and `ohe_roof_material`. You are welcome to copy over your own implementations if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_func import *    # Import functions from Project A1\n",
    "\n",
    "###### Copy any function you would like to below ######\n",
    "...\n",
    "#######################################################\n",
    "\n",
    "\n",
    "def process_data_simple(data):\n",
    "    # Remove outliers\n",
    "    data = remove_outliers(data, 'Sale Price', lower=499)\n",
    "    # Create Log Sale Price column\n",
    "    data = log_transform(data, 'Sale Price')\n",
    "    # Create Bedrooms column\n",
    "    data = add_total_bedrooms(data)\n",
    "    # Select X and y from the full data\n",
    "    X = data[['Bedrooms']]\n",
    "    y = data['Log Sale Price']\n",
    "    return X, y\n",
    "\n",
    "# Reload the data\n",
    "full_data = pd.read_csv(\"cook_county_train.csv\")\n",
    "\n",
    "# Process the data using the pipeline for the first model\n",
    "np.random.seed(1337)\n",
    "train_m1, valid_m1 = train_val_split(full_data)\n",
    "X_train_m1_simple, y_train_m1_simple = process_data_simple(train_m1)\n",
    "X_valid_m1_simple, y_valid_m1_simple = process_data_simple(valid_m1)\n",
    "\n",
    "# Take a look at the result\n",
    "display(X_train_m1_simple.head())\n",
    "display(y_train_m1_simple.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.pipe`\n",
    "\n",
    "Alternatively, we build the pipeline using `pd.DataFrame.pipe` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pipe.html)). Take a look at our use of `pd.DataFrame.pipe` below. \n",
    "\n",
    "The following function `process_data_pipe` takes in a dataframe `data`, a list `pipeline_functions` containing 3-element tuples `(function, arguments, keyword_arguments)` that will be called on `data` in the pipeline, and the label `prediction_col` that represents the column of our target variable (`Sale Price` in this case). You can use this function with each of the tuples passed in through `pipeline_functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to define process_data_pipe and select_columns, no futher actions needed.\n",
    "def process_data_pipe(data, pipeline_functions, prediction_col):\n",
    "    \"\"\"Process the data for a guided model.\"\"\"\n",
    "    for function, arguments, keyword_arguments in pipeline_functions:\n",
    "        if keyword_arguments and (not arguments):\n",
    "            data = data.pipe(function, **keyword_arguments)\n",
    "        elif (not keyword_arguments) and (arguments):\n",
    "            data = data.pipe(function, *arguments)\n",
    "        else:\n",
    "            data = data.pipe(function)\n",
    "    X = data.drop(columns=[prediction_col])\n",
    "    y = data.loc[:, prediction_col]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 2b\n",
    "\n",
    "It is time to prepare the training and validation data for the two models we proposed above. Use the following 2 cells to reload a fresh dataset from scratch and run them through the following preprocessing steps using `process_data_pipe` for each model:\n",
    "\n",
    "- Perform a `train_val_split` on the original dataset, which has been loaded as the DataFrame `full_data`. Let 80% of the set be training data and 20% of the set be validation data. \n",
    "- For both the training and validation set,\n",
    "    1. Remove outliers in `Sale Price` by so that we are considering households with a price that is strictly greater than 499 dollars (i.e., greater than or equal to 500 dollars). \n",
    "    2. Apply log transformations to `Sale Price` and the `Building Square Feet` columns to create 2 new columns `Log Sale Price` and `Log Building Square Feet`.\n",
    "    3. Extract the total number of bedrooms into a new column `Bedrooms` from the `Description` column.\n",
    "    4. Select the columns `Log Sale Price` and `Bedrooms` (and `Log Building Square Feet` as well if this is the 2nd model). We have implemented the helper function `select_columns` for you.\n",
    "    5. Return the design matrix $\\mathbb{X}$ and the observed vector $\\mathbb{Y}$. Note that $\\mathbb{Y}$ refers to the transformed `Log Sale Price`, not the original `Sale Price`. **Your design matrix and observed vector should either be numpy arrays or pandas dataframes**.\n",
    "\n",
    "Assign the final training data and validation data for both models to the following set of variables:\n",
    "\n",
    "- 1st Model: `X_train_m1`, `y_train_m1`, `X_valid_m1`, `y_valid_m1`. This is already implemented for you. \n",
    "- 2nd Model: `X_train_m2`, `y_train_m2`, `X_valid_m2`, `y_valid_m2`. Please implement this in the second cell below. You may use the first model as an example.\n",
    "\n",
    "For an example of how to work with pipelines, we have done the processing for m1 for you using `m1_pipelines` by passing in the corresponding pipeline functions as a list of tuples in the below cell. Your task is to do the same for model 2 in the cell after - that is, save your pipelines functions as a list of tuples and assign it to `m2_pipelines` for model 2.\n",
    "\n",
    "As a refresher, the equations model 1 and model 2 respectively can be found below:\n",
    "$$\n",
    "\\text{Log Sale Price} = \\theta_0 + \\theta_1 \\cdot (\\text{Bedrooms})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Log Sale Price} = \\theta_0 + \\theta_1 \\cdot (\\text{Bedrooms}) + \\theta_2 \\cdot (\\text{Log Building Square Feet})\n",
    "$$\n",
    "\n",
    "**Note**: \n",
    "* Do not change the line `np.random.seed(1337)` as it ensures we are partitioning the dataset exactly the same way for both models (otherwise their performance isn't directly comparable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data\n",
    "full_data = pd.read_csv(\"cook_county_train.csv\")\n",
    "\n",
    "# Process the data using the pipeline for the first model\n",
    "np.random.seed(1337)\n",
    "train_m1, valid_m1 = train_val_split(full_data)\n",
    "\n",
    "# Helper function\n",
    "def select_columns(data, *columns):\n",
    "    \"\"\"Select only columns passed as arguments.\"\"\"\n",
    "    return data.loc[:, columns]\n",
    "\n",
    "# Pipelines, a list of tuples\n",
    "m1_pipelines = [\n",
    "    (remove_outliers, None, {\n",
    "        'variable': 'Sale Price',\n",
    "        'lower': 499,\n",
    "    }),\n",
    "    (log_transform, None, {'col': 'Sale Price'}),\n",
    "    (add_total_bedrooms, None, None),\n",
    "    (select_columns, ['Log Sale Price', 'Bedrooms'], None)\n",
    "]\n",
    "\n",
    "X_train_m1, y_train_m1 = process_data_pipe(train_m1, m1_pipelines, 'Log Sale Price')\n",
    "X_valid_m1, y_valid_m1 = process_data_pipe(valid_m1, m1_pipelines, 'Log Sale Price')\n",
    "\n",
    "# Take a look at the result\n",
    "# It should be the same above as the result returned by process_data_simple\n",
    "display(X_train_m1.head())\n",
    "display(y_train_m1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS LINE\n",
    "np.random.seed(1337)\n",
    "# DO NOT CHANGE THIS LINE\n",
    "\n",
    "# Process the data using the pipeline for the second model\n",
    "train_m2, valid_m2 = ...\n",
    "\n",
    "m2_pipelines = ...\n",
    "\n",
    "X_train_m2, y_train_m2 = ...\n",
    "X_valid_m2, y_valid_m2 = ...\n",
    "\n",
    "\n",
    "# Take a look at the result\n",
    "display(X_train_m2.head())\n",
    "display(y_train_m2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 2c\n",
    "\n",
    "Finally, let's do some regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initialize a [`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) object for both of our models. We set the `fit_intercept = True` to ensure that the linear model has a non-zero intercept (i.e., a bias term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_m1 = lm.LinearRegression(fit_intercept=True)\n",
    "linear_model_m2 = lm.LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to fit our linear regression model. Use the cell below to fit both models, and then use it to compute the fitted values of `Log Sale Price` over the training data, and the predicted values of `Log Sale Price` for the validation data.\n",
    "\n",
    "Assign the predicted values from both of your models on the training and validation set to the following variables:\n",
    "\n",
    "- 1st Model: predicted values on **training set**: `y_fitted_m1`, predicted values on **validation set**: `y_predicted_m1`\n",
    "- 2nd Model: predicted values on **training set**: `y_fitted_m2`, predicted values on **validation set**: `y_predicted_m2`\n",
    "\n",
    "**Note**: To make sure you understand how to find the predicted value for both the training and validation data set, there won't be any hidden tests for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the 1st model\n",
    "...\n",
    "# Compute the fitted and predicted values of Log Sale Price for 1st model\n",
    "y_fitted_m1 = ...\n",
    "y_predicted_m1 = ...\n",
    "\n",
    "# Fit the 2nd model\n",
    "...\n",
    "# Compute the fitted and predicted values of Log Sale Price for 2nd model\n",
    "y_fitted_m2 = ...\n",
    "y_predicted_m2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 3: Evaluate Our Simple Model\n",
    "\n",
    "<br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 3a\n",
    "\n",
    "\n",
    "We are moving into analysis of our two models! Let's compare the performance of our two regression models using the Root Mean Squared Error (RMSE) function.\n",
    "\n",
    "$$RMSE = \\sqrt{\\dfrac{\\sum_{\\text{houses in valid set}}(\\text{actual price for house} - \\text{predicted price for house})^2}{\\text{number of of houses}}}$$\n",
    "\n",
    "The function is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculates RMSE from actual and predicted values\n",
    "    Input:\n",
    "      predicted (1D array): vector of predicted/fitted values\n",
    "      actual (1D array): vector of actual values\n",
    "    Output:\n",
    "      a float, the root-mean square error\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((actual - predicted)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 3a\n",
    "\n",
    "One way of understanding the performance (and appropriateness) of a model is through a plot of the residuals versus the observations.\n",
    "\n",
    "In the cell below, use [`plt.scatter`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) to plot the residuals from predicting `Log Sale Price` using **only the 2nd model** against the original `Log Sale Price` for the **validation data**. With a data size this large, it is difficult to avoid overplotting entirely. You should also ensure that the dot size and opacity in the scatter plot are set appropriately to reduce the impact of overplotting as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d79f42d60b94fca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "## Question 3b\n",
    "\n",
    "Based on the structure you see in your plot, does this model seem like it will correspond to _regressive_, _fair_, or _progressive_ taxation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q3b = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our simple model explains some of the variability in price, there is certainly still a lot of room for improvement to be made -- one reason is we have been only utilizing 1 or 2 features (out of a total of 70+) so far! Can you engineer and incoporate more features to improve the model's fairness and accuracy? We won't be asking you to provide your answers here, but this would be important going into the next part (also last part, wohoo!) of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 4\n",
    "\n",
    "It is time to build your own model!\n",
    "\n",
    "To evaluate your model, we will start by defining a linear regression model. Then, we will process training data using your `process_data_final` function (you will define this in 4e), fit the model with this training data, and compute the training RMSE. Then, we will process test data with your `process_data_final`, use the model to predict `Log Sale Price` for the test data, transform the predicted and original log values back into their original forms (by using `delog`), and compute the test RMSE.\n",
    "\n",
    "Your goal in Question 4 is to:\n",
    "\n",
    "* Define a function to perform feature engineering and produce a design matrix for modeling\n",
    "* Apply this feature engineering function to the training data and use it to train a model that can predict the `Log Sale Price` of houses\n",
    "* Use this trained model to predict the `Log Sale Price`s of the test set. Remember that our test set does not contain the true `Sale Price` of each house – your model is trying to guess them! \n",
    "* Submit your predicted `Log Sale Price`s on the test set to Gradescope\n",
    "\n",
    "\n",
    "In Question 4a, we will outline some important DataHub logistics.\n",
    "\n",
    "In Question 4b, you will explore possible features for your model.\n",
    "\n",
    "In Question 4c, you will perform EDA on the dataset.\n",
    "\n",
    "In Question 4d, you will define feature engineering helper functions.\n",
    "\n",
    "And, lastly, in Question 4e, you will create your design matrix and train a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Scheme\n",
    "\n",
    "Your grade for Question 4 will be based on your model’s RMSE when making predictions on the training set, as well as your model’s RMSE when making predictions on the test set. The tables below provide scoring guidelines. If your RMSE lies in a particular range, you will receive the number of points associated with that range. '\n",
    "\n",
    "**Important**: while your training RMSE can be checked at any time in this notebook, your test RMSE can only be checked by submitting your model’s predictions to Gradescope. You may only submit to Gradescope 3 times a day. Plan ahead to make sure you have enough time to finetune your model!. The thresholds are as follows:\n",
    "\n",
    "Points | 3 | 2 | 1 | 0\n",
    "--- | --- | --- | --- | ---\n",
    "Training RMSE | Less than 200k | [200k, 240k) | [240k, 280k) | More than 280k\n",
    "\n",
    "Points | 3 | 2 | 1 | 0\n",
    "--- | --- | --- | --- | ---\n",
    "Test RMSE | Less than 240k | [240k, 280k) | [280k, 300k) | More than 300k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 4a Couple notes\n",
    "\n",
    "- **If you are running into memory issues, restart kernel and only run the cells you need to.** The cell below (question cell) contains most to all of the imports necessary to successfully complete this portion of the project, so it can be completed (almost) independently code-wise from the remainder of the project. The autograder will have more than 4 GB memory, so you will not lose credit as long as your solution to Question 4 is within the total memory (4GB) limits of DataHub. Alternatively, you can delete variables you are not using through `del` or `%reset -f`. For example, this will free up memory from data used for older models: `del training_val_data, test_data, train, validation, X_train_m1, X_valid_m1, X_train_m2, X_valid_m1`. Our staff solution can be run independently from all other questions, so we encourage you to do the same to make debugging easier. Note: If you need these data again after deleting the variables or reseting, you must reload them again. In addition, you must comment them out before submitting.\n",
    "- You will be predicting `Log Sale price` on the data stored in `cook_county_contest_test.csv`. We will delog/exponentiate your prediction on Gradescope to compute RMSE and use this to score your model. Before submitting to Gradescope, make sure that your predicted values can all be delogged (i.e. if the value is 100, it is too large - $e^{100}$ is too big!)\n",
    "- You **MUST remove any additional new cells you add before submitting to Gradescope** to avoid any autograder errors. \n",
    "- You can only submit the csv file to gradescope up to **3 times** per day. Start early!\n",
    "\n",
    "**Please read the above message carefully. No response is required for this question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to clean up memory from previous questions and reinitialize Otter!\n",
    "# MAKE SURE TO COMMENT THE NEXT 3 LINES OUT BEFORE SUBMITTING!\n",
    "# %reset -f\n",
    "# del training_val_data, test_data, train, validation, X_train_m1, X_valid_m1, X_train_m2, X_valid_m2\n",
    "# import otter\n",
    "# grader = otter.Notebook(\"projA2.ipynb\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from ds100_utils import *\n",
    "from feature_func import *\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4b Finding Potential Features\n",
    "\n",
    "**This question is not graded** – it is intended to give helpful guidance on how to get started with feature engineering in Q4e. You may write as little or as much as you would like here; it will not factor into your grade. Read the documentation about the dataset in `codebook.txt`, located in this directory. Is there any data you think that may be related to housing price? Drop them down below for your later reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4c More EDA\n",
    "\n",
    "\n",
    "**This question is not graded** – it is intended to give helpful guidance on how to get started with feature engineering in Q4e. You may write as little or as much as you would like here; it will not factor into your grade. Use the scratch space below to conduct any additional EDA you would like to see. You may use this space to make additional plots to help you visualize the relationship between any variables or compute any relevant statistics. You are free to add any number of cells as needed below and before the next question. You may find it helpful to review Project A1 and techniques we explore there.\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>[<b>Click to Expand</b>] Some potential ideas. </summary>\n",
    "    \n",
    "* Plot the distribution of a variable. Is this variable heavily skewed? Are there any outliers?\n",
    "\n",
    "* Make a scatter plot between a continous feature and the outcome. Is there a relationship? Is there a transformation that may linearize the relationship?\n",
    "\n",
    "* Make a plot of a categorical/discrete feature and the outcome. Is there a relationship? How can we transform this categorical data into numerical features that can be useful for OLS?\n",
    "\n",
    "* Find the correlation coefficient between features and the outcome. Is there a strong relationship between the two? Can you find the correlation coefficient between different transformations of the feature and the outcome?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add any EDA code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add any EDA code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add any EDA code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add any EDA code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add any EDA code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4d Defining Helper Function or Helper Variables\n",
    "\n",
    "**This question is not graded but we suggest that you put all your helper functions below for readability and ease of testing.** Use this space below to define any additional helper functions you may use in your final model. These can be transformation functions you identified in the optional question above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define any additional helper functions or variables you need here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4e Defining Pipeline Funtion\n",
    "\n",
    "Just as in the guided model from the previous question, you should encapsulate as much of your workflow into functions as possible. Your job is to select better features and define your own feature engineering pipeline inside the function `process_data_final` in the following cell. Use of `.pipe` is not required, but you are welcome to incorporate it! **You must not change the parameters inside `process_data_final`. Do not edit the two lines at the end of the question cell below. They are helper function that defined a linear model, fit your data, and compute RMSE. If you do, you will receive no credit for this question.** \n",
    "\n",
    "- Any feature engineering techniques that involve referencing `Sale Price` (for example, removing outlying `Sale Price` values from the training data) should be performed under the condition `if not is_test_set:`\n",
    "- All other feature engineering techniques should be applied to both the training and test sets. This means that you should perform them under the condition `else:`\n",
    "- When `is_test_set` is `True`, your function should return only the design matrix, `X`.\n",
    "- When `is_test_set` is `False`, your function should return both the design matrix and the response variable `y` (the `Log Sale Price` column).\n",
    "\n",
    "\n",
    "Hinst:\n",
    "-  Some features may have missing values in the test set but not in the training/validation set. Make sure `process_data_final` handles missing values appropriately for each feature\n",
    "- We have imported all feature engineering functions from Project A1 for you. You can view them in `feature_func.py`, which can be accesssed through the directory available on the left in File Browser.\n",
    "- You may wish to consider removing outlying datapoints from the training set before fitting your model. You may not, however, remove any datapoints from the test set (the CCAO could not simply \"refuse\" to make a prediction for a particular house!)\n",
    "- As you finetune your model, you may unintentionally consume too much DataHub memory, causing your kernel to crash. See Question 4a for guidance on how to resolve this.\n",
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please include all of your feature engineering process inside this function.\n",
    "# Do not modify the parameters of this function.\n",
    "def process_data_final(data, is_test_set=False):\n",
    "    # Whenever you access 'Log Sale Price' or 'Sale Price', make sure to use the\n",
    "    # condition is_test_set like this:\n",
    "    if not is_test_set:\n",
    "        # do your processing for the training set (i.e. not the test set)\n",
    "        # this can involve references to sale price!\n",
    "        data['Log Sale Price'] = np.log(data['Sale Price'])\n",
    "        ...\n",
    "    else:\n",
    "        ...\n",
    "        # do your processing for the test set\n",
    "        # this CANNOT involve references to sale price!\n",
    "    \n",
    "    # do your processing for the both train and test set\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    # Return predictors and response variable separately\n",
    "    if is_test_set:\n",
    "        # Optional processing you wish to do, remove ellipsis before submission\n",
    "        ... \n",
    "        # Predictors\n",
    "        X = ...\n",
    "        return X\n",
    "    else:\n",
    "        # Optional processing you wish to do, remove ellipsis before submission\n",
    "        ... \n",
    "        # Predictors. Your X should not include Log Sale Price!\n",
    "        X = ...\n",
    "        # Response variable\n",
    "        y = ...\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "# DO NOT EDIT THESE TWO LINES!\n",
    "check_rmse_threshold = run_linear_regression_test_optim(lm.LinearRegression(fit_intercept=True), process_data_final, 'cook_county_train.csv', None, False)\n",
    "print(\"Current training RMSE:\", check_rmse_threshold.loss)\n",
    "print(\"You can check your grade for your prediction as per the grading scheme outlined at the start of Q4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4f Fit and Evaluate your Model\n",
    "\n",
    "**This question is not graded.** Use this space below to evalute your models. Some ideas are listed below. \n",
    "\n",
    "**Note:** While we have grader function that checks RMSE for you, it is best to define create your own model object and fit on your data. This way, you have access to the model directly to help you evaluate/debug if needed. For this project, you should use a sklearn default `LinearRegression()` model with intercept term for grading purposes. Do not modify any hyperparameter in `LinearRegression()`, and focus on feature selection or hyperpameters of your own feature engineering function.\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>[<b>Click to Expand</b>] Hints: </summary>\n",
    "    \n",
    "Train set:\n",
    "\n",
    "* Check your test RMSE. Is this a reasonable number? You may use our grading scheme as reference. Keep in mind that training error is generally less than testing error. \n",
    "\n",
    "Test set:\n",
    "* Find the original data shape at the begining of the notebook (in the provided assert statement). What should the output shape be?\n",
    "\n",
    "* Since test and training/validation sets comes from the same population (recall that test and training/validation sets are a random split from a larger data), we expect our test prediction to have a similar range as the training data. Plot the observed training (Log) Sale Price and the predicted (Log) Sale Price. Are the ranges similar? Do you have any unreasonable extreme prediction that cannot be exponentiated?\n",
    "\n",
    "* We cannot compute test RMSE directly since we do not have the observed values. Perform a holdout-test or cross validation (on training/validation sets) to estimate your test error. Recall that we are treating validation set as unseen data.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use this space to evaluate your model\n",
    "# if you reset your memory, you need to define the functions again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4g Submission\n",
    "\n",
    "Recall that the test set given to you in this assignment does not contain values for the true `Sale Price` of each house. To determine your model’s RMSE on the test set, you will submit the predictions made by your model to Gradescope. There, we will run checks to see what your test RMSE is by considering (hidden) true values for the `Sale Price`.\n",
    "Your score on this section will be determined by the grading scheme outlined at the start of Question 4. Remember that you can only submit test predictions to Gradescope 3 times per day. Plan your time to ensure that you can adjust your model as necessary.\n",
    "\n",
    "To determine the error on the test set, please submit your predictions on the contest test set to the Gradescope assignment: **Project A2 Test Set Predictions**. The CSV file to submit is generated below and you should not modify the cell below. Simply download the CSV file and submit it to the appropriate Gradescope assignment.\n",
    "\n",
    "Note that **you will not receive credit for the test set predictions (i.e. up to 3 points) unless you submit to this assignment**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "Y_test_pred = run_linear_regression_test(lm.LinearRegression(fit_intercept=True), process_data_final, None, 'cook_county_train.csv', 'cook_county_contest_test.csv', \n",
    "                                         is_test = True, is_ranking = False, return_predictions = True\n",
    "                                         )\n",
    "\n",
    "# Construct and save the submission:\n",
    "submission_df = pd.DataFrame({\n",
    "    \"Id\": pd.read_csv('cook_county_contest_test.csv')['Unnamed: 0'], \n",
    "    \"Value\": Y_test_pred,\n",
    "}, columns=['Id', 'Value'])\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = \"submission_{}.csv\".format(timestamp)\n",
    "submission_df.to_csv(filename, index=False)\n",
    "\n",
    "#print('Created a CSV file: {}.'.format(\"submission_{}.csv\".format(timestamp)))\n",
    "display(HTML(\"Download your test prediction <a href='\" + filename + \"' download>here</a>.\"))\n",
    "print('You may now upload this CSV file to Gradescope for scoring.')#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch space to check if your prediction is reasonable. See 4f for hints. \n",
    "# We will not reset submission count for mis-submission issues.\n",
    "submission_df[\"Value\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing your prediction model for home sale prices in Cook County! In the following section, we'll delve deeper into the implications of predictive modeling within the CCAO case study - especially because statistical modeling is how the CCAO valuates properties. \n",
    "\n",
    "Refer to Lecture 14 if you're having trouble getting started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## (Optional) Question 5 Evaluating Model in Context\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## (Optional) Question 5a\n",
    "\n",
    "When evaluating your model, we used root mean squared error. In the context of estimating the value of houses, what does residual mean for an individual homeowner? How does it affect them in terms of property taxes? Discuss the cases where residual is positive and negative separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the Cook County Assessor’s Office, Chief Data Officer Rob Ross states that fair property tax rates are contingent on whether property values are assessed accurately - that they’re valued at what they’re worth, relative to properties with similar characteristics. This implies that having a more accurate model results in fairer assessments. The goal of the property assessment process for the CCAO, then, is to be as accurate as possible. \n",
    "\n",
    "When the use of algorithms and statistical modeling has real-world consequences, we often refer to the idea of fairness as a measurement of how socially responsible our work is. But fairness is incredibly multifaceted: Is a fair model one that minimizes loss - one that generates accurate results? Is it one that utilizes \"unbiased\" data? Or is fairness a broader goal that takes historical contexts into account?\n",
    "\n",
    "These approaches to fairness are not mutually exclusive. If we look beyond error functions and technical measures of accuracy, we'd not only consider _individual_ cases of fairness, but also what fairness - and justice - means to marginalized communities on a broader scale. We'd ask: What does it mean when homes in predominantly Black and Hispanic communities in Cook County are consistently overvalued, resulting in proportionally higher property taxes? When the white neighborhoods in Cook County are consistently undervalued, resulting in proportionally lower property taxes? \n",
    "\n",
    "Having \"accurate\" predictions doesn't necessarily address larger historical trends and inequities, and fairness in property assessments in taxes works beyond the CCAO's valuation model. Disassociating accurate predictions from a fair system is vital to approaching justice at multiple levels. Take Evanston, IL - a suburb in Cook County - as an example of housing equity beyond just improving a property valuation model: Their City Council members [recently approved reparations for African American residents](https://www.usnews.com/news/health-news/articles/2021-03-23/chicago-suburb-approves-government-reparations-for-black-residents).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## (Optional) Question 5b\n",
    "\n",
    "In your own words, describe how you would define fairness in property assessments and taxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CCAO and Transparency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, in their approach to fair property valuations, the CCAO has also pushed for transparency initiatives in the property tax assessment system. After a lawsuit was filed against the CCAO for producing [“racially discriminatory assessments and taxes,\"](https://harris.uchicago.edu/news-events/news/prof-chris-berry-testifies-institutional-racism-cook-county-property-taxes) the Office decided that these inequities would be best addressed by making the assessment process more transparent to Cook County constituents.  \n",
    "\n",
    "These transparency initiatives include publishing all of the CCAO’s work on [GitLab](https://gitlab.com/ccao-data-science---modeling). By allowing the public to access any updates to the system in real-time, the Office argues that they increase accessibility to a process that had previously been blackboxed - obscured and hidden - from the public. Ultimately, the hope is that, by exposing the inner workings of the CCAO’s property valuation process, the CCAO's assessment results could be publicly verified as accurate and therefore trusted to be fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## (Optional) Question 5c\n",
    "\n",
    "Take a look at the Residential Automated Valuation Model files under the Models subgroup in the CCAO’s [GitLab](https://gitlab.com/ccao-data-science---modeling). Without directly looking at any code, do you feel that the documentation sufficiently explains how the residential valuation model works? Which part(s) of the documentation might be difficult for nontechnical audiences to understand?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might feel that the model's inner workings are beyond your pay grade - it's far more complex than the model you built in this assignment, after all! Though we won't delve further into the role of transparency in the broader CCAO case study, consider its effectiveness and/or ineffectiveness: Is the system truly transparent if it's inaccessible to Cook County constituents? Do transparency measures actually bolster the accuracy of a model - or do they only affect the _perceived_ accuracy of a model? \n",
    "\n",
    "And if you're interested in thinking more about transparency measures, take Data 104! But for now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Congratulations! You have finished Project A2!\n",
    "\n",
    "Below, you will see two cells. Running the first cell will automatically generate a PDF of all questions that need to be manually graded, and running the second cell will automatically generate a zip with your autograded answers. **You are responsible for both the coding portion (the zip from Project A2) and the written portion (the PDF with from Project A2) to their respective Gradescope portals, and checking that they are the most recent copy or the copy you wish to submit (including plots).** The coding proportion should be submitted to Project A2 Coding as a single zip file, and the written portion should be submitted to Project A2 Written as a single pdf file. When submitting the written portion, please ensure you select pages appropriately. \n",
    "\n",
    "If there are issues with automatically generating the PDF in the first cell, you can try downloading the notebook as a PDF by clicking on `File -> Save and Export Notebook As... -> PDF`. If that doesn't work either, you can manually take screenshots of your answers to the manually graded questions and submit those. Either way, **you are responsible for ensuring your submission follows our requirements, we will NOT be granting regrade requests for submissions that don't follow instructions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from otter.export import export_notebook\n",
    "from os import path\n",
    "from IPython.display import display, HTML\n",
    "export_notebook(\"projA2.ipynb\", filtering=True, pagebreaks=True)\n",
    "if(path.exists('projA2.pdf')):\n",
    "    display(HTML(\"Download your PDF <a href='projA2.pdf' download>here</a>.\"))\n",
    "else:\n",
    "    print(\"\\n Pdf generation fails, please try the other methods described above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q0c": {
     "name": "q0c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q0c.lower() in ['a', 'b', 'c']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1": {
     "name": "q1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> train.shape == (163833, 62) # Train should contain 80% of the data\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> validation.shape == (40959, 62) # Validation should contain 20% of the data\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(train[\"Sale Price\"].mean(), 244939.22668204817, atol=0.1) # If this doesn't match, you might have still answered the question, but please adjust your code so that your split matches ours by following the implementation instructions about using shuffled_indices to split the data.\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.allclose(validation.index[-5:], [153946, 117415, 9448, 188605, 3223])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(validation[\"Sale Price\"].mean(), 246066.1821089382, atol=0.1) # If this doesn't match, you might have still answered the question, but please adjust your code so that your split matches ours by following the implementation instructions about using shuffled_indices to split the data.\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (q2a in ['>=', '=', '<=']) == True\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q2a == '>=' # HIDDEN\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (isinstance(X_train_m1, pd.core.frame.DataFrame)) and \\\n... (isinstance(y_train_m1, pd.core.series.Series)) and \\\n... (isinstance(X_valid_m1, pd.core.frame.DataFrame)) and \\\n... (isinstance(y_valid_m1, pd.core.series.Series)) and \\\n... (isinstance(X_train_m2, pd.core.frame.DataFrame)) and \\\n... (isinstance(y_train_m2, pd.core.series.Series)) and \\\n... (isinstance(X_valid_m2, pd.core.frame.DataFrame)) and \\\n... (isinstance(y_valid_m2, pd.core.series.Series))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert len(m2_pipelines) == 5\n>>> assert log_transform in set([p[0] for p in m2_pipelines])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(y_fitted_m1.max(), 17.528601849438104, atol=0.0001) == True\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(y_fitted_m2.max(), 15.614096224439168, atol=0.0001) == True\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(y_predicted_m1.max(), 15.540922864181525, atol=0.0001) == True\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(y_predicted_m2.max(), 15.02563963305767, atol=0.0001) == True\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q3b.lower() in [\"regressive\", \"fair\", \"progressive\"]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4e": {
     "name": "q4e",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> check_rmse_threshold(200000)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> check_rmse_threshold(240000)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> check_rmse_threshold(280000)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> check_rmse_threshold.signature == (process_data_final, 'cook_county_train.csv', None)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
